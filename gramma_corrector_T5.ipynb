{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1897f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datasets\n",
    "import pandas as pd\n",
    "from types import SimpleNamespace\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "\n",
    "from GrammarCorrector.utils import GrammarDataset\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "from datasets import Dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ba3dfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set params\n",
    "params = SimpleNamespace()\n",
    "params.max_len = 121\n",
    "params.model_name = 't5-base'\n",
    "params.batch_size = 32\n",
    "params.num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b38eadf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set seed\n",
    "def set_seed(seed):\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "tqdm.pandas()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4425777",
   "metadata": {},
   "source": [
    "# Data prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6603de7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The steps below describe how to remove data for one or more specifies areas and how to put on the data from a snapshot to the index</td>\n",
       "      <td>The steps below describe how to remove data for one ore more specific areas and how to put back the data from a snapshot to the index.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I wake up it\\'s usually comes out dreamsI\\'m thinking so my thoughts are very weird.</td>\n",
       "      <td>When I wake up it\\'s usually dreams I\\'m thinking about so my thoughts are very weird.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One of the cardinal factors to be considered trying to decide on which kind of shipping to customer settle is the! market difference.</td>\n",
       "      <td>One of the cardinal factors to consider when trying to decide on which kind of shipping to settle for is the market difference.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Answers » Regions » Is in Nagorno-Karabakt region that part in Armenia?</td>\n",
       "      <td>Answers » Regions » Is Nagorno-Karabakh region part of Armenia?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Flaneuring in fun at maple creek SK!</td>\n",
       "      <td>Flaneuring Fun in Maple Creek SK!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549995</th>\n",
       "      <td>Despite before the counter-offensive launch, Kunduz swarmed with Taliban fighters racing stolen police vehicles and vans of Red Cross.</td>\n",
       "      <td>Despite the launch of the counter-offensive, Kunduz swarmed with Taliban fighters racing stolen police vehicles and Red Cross vans.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549996</th>\n",
       "      <td>A spokesman said; \" Bad weather on its way today, so anyone on the roads be mindful of changing conditions.\"</td>\n",
       "      <td>A spokesman said: \"Bad weather on its way today, so anyone on the roads be mindful of changing conditions.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549997</th>\n",
       "      <td>2) Click on Get to Site Administration here.</td>\n",
       "      <td>2) Click on Go to Site Administration.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549998</th>\n",
       "      <td>Habits/Hobbies likes to make friends, colects gems and shiny treasures.</td>\n",
       "      <td>Habits/Hobbies: Likes to make friends, Collects Gems and Shiny Treasures.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549999</th>\n",
       "      <td>AuthHelper 1279 3 years bachuck Upped max asize for saved prefs.</td>\n",
       "      <td>AuthHelper 1279 3 years speck Upped max size for saved prefs.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>550000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                         input  \\\n",
       "0          The steps below describe how to remove data for one or more specifies areas and how to put on the data from a snapshot to the index   \n",
       "1                                                    When I wake up it\\'s usually comes out dreamsI\\'m thinking so my thoughts are very weird.   \n",
       "2        One of the cardinal factors to be considered trying to decide on which kind of shipping to customer settle is the! market difference.   \n",
       "3                                                                      Answers » Regions » Is in Nagorno-Karabakt region that part in Armenia?   \n",
       "4                                                                                                         Flaneuring in fun at maple creek SK!   \n",
       "...                                                                                                                                        ...   \n",
       "549995  Despite before the counter-offensive launch, Kunduz swarmed with Taliban fighters racing stolen police vehicles and vans of Red Cross.   \n",
       "549996                            A spokesman said; \" Bad weather on its way today, so anyone on the roads be mindful of changing conditions.\"   \n",
       "549997                                                                                            2) Click on Get to Site Administration here.   \n",
       "549998                                                                 Habits/Hobbies likes to make friends, colects gems and shiny treasures.   \n",
       "549999                                                                        AuthHelper 1279 3 years bachuck Upped max asize for saved prefs.   \n",
       "\n",
       "                                                                                                                                        output  \n",
       "0       The steps below describe how to remove data for one ore more specific areas and how to put back the data from a snapshot to the index.  \n",
       "1                                                       When I wake up it\\'s usually dreams I\\'m thinking about so my thoughts are very weird.  \n",
       "2              One of the cardinal factors to consider when trying to decide on which kind of shipping to settle for is the market difference.  \n",
       "3                                                                              Answers » Regions » Is Nagorno-Karabakh region part of Armenia?  \n",
       "4                                                                                                            Flaneuring Fun in Maple Creek SK!  \n",
       "...                                                                                                                                        ...  \n",
       "549995     Despite the launch of the counter-offensive, Kunduz swarmed with Taliban fighters racing stolen police vehicles and Red Cross vans.  \n",
       "549996                             A spokesman said: \"Bad weather on its way today, so anyone on the roads be mindful of changing conditions.\"  \n",
       "549997                                                                                                  2) Click on Go to Site Administration.  \n",
       "549998                                                               Habits/Hobbies: Likes to make friends, Collects Gems and Shiny Treasures.  \n",
       "549999                                                                           AuthHelper 1279 3 years speck Upped max size for saved prefs.  \n",
       "\n",
       "[550000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./DATA/c4_200m_550k.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3976448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(params.model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(params.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cbd918c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3173/550000 [00:00<00:51, 10715.10it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1117 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 550000/550000 [00:51<00:00, 10595.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# get max len of tokenized sentences\n",
    "def calc_token_len(example):\n",
    "    return len(tokenizer(example).input_ids)\n",
    "\n",
    "df['input_token_len'] = df['input'].progress_apply(calc_token_len)\n",
    "params.max_len = df['input_token_len'].describe(percentiles=[0.99])['99%']\n",
    "df.drop('input_token_len', axis=1, inplace=True)\n",
    "print(params.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87ae297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to test and train, go to datasets\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, shuffle=True)\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de4aca4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids 20\n",
      "attention_mask 20\n",
      "labels 24\n",
      "{'input_ids': [71, 973, 24, 14079, 24067, 38, 96, 77, 221, 3728, 121, 19, 59, 2930, 7509, 640, 569, 2287, 5, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [71, 973, 24, 14079, 3, 89, 12578, 887, 21, 96, 77, 221, 3728, 121, 3270, 19, 59, 2930, 7509, 640, 569, 2287, 5, 1]}\n"
     ]
    }
   ],
   "source": [
    "dataset = GrammarDataset(test_dataset, tokenizer, params.max_len, True)\n",
    "print(dataset[121])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10af18cb",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95e11eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bitzh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "rouge_metric = load_metric(\"rouge\")\n",
    "nltk.download('punkt')\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model,\n",
    "                                       padding='longest', return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9f7c8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Seq2SeqTrainingArguments(output_dir=\"DATA/weights\",\n",
    "                        evaluation_strategy=\"epoch\",\n",
    "                        save_strategy=\"epoch\",\n",
    "                        per_device_train_batch_size=params.batch_size,\n",
    "                        per_device_eval_batch_size=params.batch_size,\n",
    "                        learning_rate=2e-5,\n",
    "                        num_train_epochs=params.num_epochs,\n",
    "                        weight_decay=0.01,\n",
    "                        save_total_limit=2,\n",
    "                        predict_with_generate=True,\n",
    "                        fp16 = True,\n",
    "                        gradient_accumulation_steps = 6,\n",
    "#                         eval_steps = 500,\n",
    "#                         save_steps = 500,\n",
    "                        load_best_model_at_end=True,\n",
    "                        logging_dir=\"/logs\",\n",
    "                        report_to=\"wandb\")\n",
    "\n",
    "# metric from the arcticle\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return {k: round(v, 4) for k, v in result.items()}\n",
    "\n",
    "# defining trainer using 🤗\n",
    "trainer = Seq2SeqTrainer(model=model, \n",
    "                args=args, \n",
    "                train_dataset= GrammarDataset(train_dataset, tokenizer),\n",
    "                eval_dataset=GrammarDataset(test_dataset, tokenizer),\n",
    "                tokenizer=tokenizer,\n",
    "                data_collator=data_collator,\n",
    "                compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4131a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 495000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 192\n",
      "  Gradient Accumulation steps = 6\n",
      "  Total optimization steps = 7734\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mresquilleur\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\bitzh\\Documents\\JupiterNotebooks\\GrammarCorrector\\wandb\\run-20220413_095109-1rsps01c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/resquilleur/huggingface/runs/1rsps01c\" target=\"_blank\">DATA/weights</a></strong> to <a href=\"https://wandb.ai/resquilleur/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7734' max='7734' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7734/7734 3:59:43, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.612300</td>\n",
       "      <td>0.558183</td>\n",
       "      <td>72.178900</td>\n",
       "      <td>62.365800</td>\n",
       "      <td>71.476400</td>\n",
       "      <td>71.502700</td>\n",
       "      <td>17.245600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.592600</td>\n",
       "      <td>0.542531</td>\n",
       "      <td>72.345700</td>\n",
       "      <td>62.670800</td>\n",
       "      <td>71.649400</td>\n",
       "      <td>71.678000</td>\n",
       "      <td>17.229200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.583200</td>\n",
       "      <td>0.539419</td>\n",
       "      <td>72.393100</td>\n",
       "      <td>62.752400</td>\n",
       "      <td>71.702200</td>\n",
       "      <td>71.729700</td>\n",
       "      <td>17.225600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 55000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to DATA/weights\\checkpoint-2578\n",
      "Configuration saved in DATA/weights\\checkpoint-2578\\config.json\n",
      "Model weights saved in DATA/weights\\checkpoint-2578\\pytorch_model.bin\n",
      "tokenizer config file saved in DATA/weights\\checkpoint-2578\\tokenizer_config.json\n",
      "Special tokens file saved in DATA/weights\\checkpoint-2578\\special_tokens_map.json\n",
      "Copy vocab file to DATA/weights\\checkpoint-2578\\spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 55000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to DATA/weights\\checkpoint-5156\n",
      "Configuration saved in DATA/weights\\checkpoint-5156\\config.json\n",
      "Model weights saved in DATA/weights\\checkpoint-5156\\pytorch_model.bin\n",
      "tokenizer config file saved in DATA/weights\\checkpoint-5156\\tokenizer_config.json\n",
      "Special tokens file saved in DATA/weights\\checkpoint-5156\\special_tokens_map.json\n",
      "Copy vocab file to DATA/weights\\checkpoint-5156\\spiece.model\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 55000\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to DATA/weights\\checkpoint-7734\n",
      "Configuration saved in DATA/weights\\checkpoint-7734\\config.json\n",
      "Model weights saved in DATA/weights\\checkpoint-7734\\pytorch_model.bin\n",
      "tokenizer config file saved in DATA/weights\\checkpoint-7734\\tokenizer_config.json\n",
      "Special tokens file saved in DATA/weights\\checkpoint-7734\\special_tokens_map.json\n",
      "Copy vocab file to DATA/weights\\checkpoint-7734\\spiece.model\n",
      "Deleting older checkpoint [DATA\\weights\\checkpoint-2578] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from DATA/weights\\checkpoint-7734 (score: 0.539419412612915).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7734, training_loss=0.6141996026069951, metrics={'train_runtime': 14393.5347, 'train_samples_per_second': 103.171, 'train_steps_per_second': 0.537, 'total_flos': 1.12537321168896e+17, 'train_loss': 0.6141996026069951, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a6fdc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to t5_gec_model\n",
      "Configuration saved in t5_gec_model\\config.json\n",
      "Model weights saved in t5_gec_model\\pytorch_model.bin\n",
      "tokenizer config file saved in t5_gec_model\\tokenizer_config.json\n",
      "Special tokens file saved in t5_gec_model\\special_tokens_map.json\n",
      "Copy vocab file to t5_gec_model\\spiece.model\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('t5_gec_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183e03c4",
   "metadata": {},
   "source": [
    "# Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d156942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = T5Tokenizer.from_pretrained(params.model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5_gec_model').to(device).eval()\n",
    "\n",
    "def correct_grammar(input_text,num_return_sequences):\n",
    "  batch = tokenizer([input_text],truncation=True,padding='max_length',max_length=64, return_tensors=\"pt\").to(torch_device)\n",
    "  translated = model.generate(**batch,max_length=64,num_beams=4, num_return_sequences=num_return_sequences, temperature=1.5)\n",
    "  tgt_text = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "  return tgt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "422319ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He is moving here.', 'He is pls moving here.']\n"
     ]
    }
   ],
   "source": [
    "text = 'He are pl moving here.'\n",
    "print(correct_grammar(text, num_return_sequences=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1d33963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cats do not drink milk.', 'Cats are not drinking milk.', 'Cats are not drink milk.']\n"
     ]
    }
   ],
   "source": [
    "text = 'Cat are not drinked milk'\n",
    "print(correct_grammar(text, num_return_sequences=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zhuk_home",
   "language": "python",
   "name": "zhuk_home"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
